data:
  name: mmWavoice
  vocab: /data/home/Tiantian_Liu/data/mmWavoice_dataset/processed/idx2chap.csv  
  batch_size: 8 #32
  text: character
  train: /data/home/Tiantian_Liu/data/mmWavoice_dataset/processed/train.csv 
  test: /data/home/Tiantian_Liu/data/mmWavoice_dataset/processed/dev.csv 
  dev: /data/home/Tiantian_Liu/data/mmWavoice_dataset/processed/dev.csv 
  short_first: False
  num_mel_bins: 40 # 40
  num_works: 8
  vocab_size: 29 #30
model:
  listener:
    input_feature_dim: 40
    hidden_size: 128
    num_layers: 2
    dropout: 0.0
    bidirectional: True
    rnn_unit: "LSTM"
    use_gpu: True
  speller:
    hidden_size: 256
    num_layers: 2
    bidirectional: True
    rnn_unit: "LSTM"
    vocab_size: 29         #30                   
    multi_head: 1                               
    decode_mode: 1                              
    use_mlp_in_attention: True                  
    mlp_dim_in_attention: 64                   
    mlp_activate_in_attention: 'relu'           
    listener_hidden_size: 128
    max_label_len: 576
training:
  optimizer: 'adam'
  lr: 0.001 #0.0002
  weight_decay: 0.000
  momentum: 0.0
  epochs: 60 #100
  half_lr: 0.0
  early_stop: 0.0
  max_norm: 5
  save_folder: 'b1_runs0422/' #'runs0111/'
  checkpoint: True
  continue_from: False #runs/-epoch4.pth.tar
  tensorboard: True
  print_freq: 500
  label_smoothing: 0.1
  tf_rate_upperbound: 0.9  #0.9                   # teacher forcing rate during training will be linearly
  tf_rate_lowerbound: 0.5                    # decaying from upperbound to lower bound for each epoch
  tf_decay_step: 100000
